{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyFq3m0WxcMH"
      },
      "source": [
        "# Transformer Sign2text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxhddjUuxcML"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dsJwcirH2Pa",
        "outputId": "8d8e7965-04da-4408-b53f-5d64b0dcb1f7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-text==2.15.0\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.15.0) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (4.12.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.13.0->tensorflow-text==2.15.0) (2.15.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-text==2.15.0\n",
        "#!pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU_bI0D7xcMM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_text as text\n",
        "import string\n",
        "import re\n",
        "import zipfile\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "import pathlib\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "pwd = pathlib.Path.cwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aRXGdCtxwgr",
        "outputId": "173581e1-6b27-436b-c36a-84777990d92c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "DIR = \"H2S2/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "f938122b5c6c4ad8b397ababbd0b2390",
            "8d2872e0ee904d88a8934cf0b4280d78",
            "96f63b3371444414a3fded94f5b7bd29",
            "8be432fe40894001a4786e04e19e3e87",
            "4693d5d2894746e9987dce61f38759e8",
            "f5ded3649a194f6d82efcccf011f012e",
            "4c5ded9a701c4672a938fc4d6055495c",
            "7958d4ddb43143fb9536d1fc39cb91cb",
            "4c19672d3d5444a0905dadff39f977a4",
            "ae503f0b24ff4fb5ae0cdf87d75cd8c9",
            "311496315ec8492b9a15d0827753c797"
          ]
        },
        "id": "UaHgQWXfDcUn",
        "outputId": "2c065b71-c1df-410a-8223-0bc1c2c5c052"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extraindo:   0%|          | 0.00/2.68G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f938122b5c6c4ad8b397ababbd0b2390"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processo concluído.\n"
          ]
        }
      ],
      "source": [
        "# Caminho para o arquivo local no Google Drive\n",
        "path_to_zip = \"/content/drive/MyDrive/H2S2.zip\"\n",
        "\n",
        "# Função para extrair com barra de progresso\n",
        "def extract_with_progress(zip_path, output_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # Lista dos nomes dos arquivos no zip\n",
        "        zip_files = [zinfo.filename for zinfo in zip_ref.filelist]\n",
        "        # Verifica se pelo menos um arquivo já existe no diretório de destino\n",
        "        already_exists = any(os.path.exists(os.path.join(output_path, f)) for f in zip_files)\n",
        "\n",
        "        if already_exists:\n",
        "            print(\"Alguns arquivos já existem no diretório de destino. Extração pulada.\")\n",
        "            return\n",
        "\n",
        "        # Cria um objeto tqdm para a barra de progresso\n",
        "        total = sum([zinfo.file_size for zinfo in zip_ref.filelist])\n",
        "        with tqdm(total=total, unit='B', unit_scale=True, desc=\"Extraindo\") as pbar:\n",
        "            for zinfo in zip_ref.filelist:\n",
        "                zip_ref.extract(zinfo, output_path)\n",
        "                pbar.update(zinfo.file_size)\n",
        "\n",
        "extract_with_progress(path_to_zip, \"/content/\")  # Extrair para a pasta do Google Colab\n",
        "print(\"Processo concluído.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI-OOcSl5z3h"
      },
      "outputs": [],
      "source": [
        "WORKSPACE = \"SENTENCE\"\n",
        "tfrecord = 'tfrecord/'\n",
        "\n",
        "BATCH_SIZE = 60\n",
        "\n",
        "N_EPOCHS = 120\n",
        "FRAME_LEN = 180\n",
        "MAX_TOKENS = 30\n",
        "\n",
        "lr = 1e-4\n",
        "wd = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b48zhu0KxcMO"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuFgswGtxcMO"
      },
      "outputs": [],
      "source": [
        "# Define the _parse_function to read and parse the TFRecord files\n",
        "def _parse_function(proto):\n",
        "    # Define the feature description dictionary\n",
        "    feature_description = {\n",
        "        'keypoints': tf.io.VarLenFeature(tf.float32),\n",
        "        'caption': tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "    # Parse the input tf.train.Example proto using the dictionary above\n",
        "    parsed_features = tf.io.parse_single_example(proto, feature_description)\n",
        "    # Reshape frames based on the actual shape of the frames in your dataset\n",
        "    frames = tf.reshape(tf.sparse.to_dense(parsed_features['keypoints']), [-1, 59, 3])\n",
        "    caption = parsed_features['caption']\n",
        "    return frames, caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlwvE5XexcMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94e80164-6b0f-4337-bf33-7e6ba3223714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(142, 59, 3) b'This is all the you know, take off on the idea of the acanthus leaf'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30159"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "def load_dataset(tfrecord_path):\n",
        "    dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
        "    dataset = dataset.map(_parse_function)\n",
        "    return dataset\n",
        "\n",
        "# Load the datasets\n",
        "train_raw = load_dataset(f'{tfrecord}train.tfrecord')\n",
        "dev_raw = load_dataset(f'{tfrecord}dev.tfrecord')\n",
        "test_raw = load_dataset(f'{tfrecord}test.tfrecord')\n",
        "\n",
        "train_raw = train_raw.concatenate(dev_raw).concatenate(test_raw)\n",
        "\n",
        "# Example usage: Iterate over the dataset\n",
        "for frames, caption in train_raw.take(1):\n",
        "    print(frames.shape, caption.numpy())\n",
        "\n",
        "len(list(train_raw))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skoAArDhxcMS"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wfP_obwbHTU"
      },
      "source": [
        "### Import tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzeIpTJJIYAM"
      },
      "outputs": [],
      "source": [
        "model_name = WORKSPACE\n",
        "tokenizers = tf.saved_model.load(model_name)\n",
        "tokenizer = tokenizers.en\n",
        "pad_token_idx = 0\n",
        "\n",
        "@tf.function\n",
        "def tokenize_caption(caption):\n",
        "    caption_sequence = tokenizers.en.tokenize([caption])[0]\n",
        "    # Truncar ou fazer padding de acordo com o comprimento máximo de 80 tokens\n",
        "    padded_sequence = caption_sequence[:MAX_TOKENS]\n",
        "    # Preencher com zeros se a sequência for menor que MAX_TOKENS\n",
        "    padded_sequence = tf.pad(padded_sequence, paddings=[[0, MAX_TOKENS - tf.shape(padded_sequence)[0]]], constant_values=0)\n",
        "    return padded_sequence\n",
        "\n",
        "@tf.function\n",
        "def tokenize_function(path, caption):\n",
        "    tokenized_caption = tokenize_caption(caption)\n",
        "    return path, tokenized_caption"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizers.en.get_vocab_size().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_N57D-qKQl3",
        "outputId": "9d18f9db-fb35-4cb5-fb08-cc02e12c4424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4446"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc93Gvz0dmsD"
      },
      "source": [
        "## Landmarks Pre Proces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl2eP9xSdrp5"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def spatial_random_affine(keypoints,\n",
        "                          scale_range = (0.8, 1.2),\n",
        "                          rotation_range = (-30, 30),\n",
        "                          shear_range = (-0.15, 0.15),\n",
        "                          translation_range = (-0.1, 0.1)):\n",
        "\n",
        "    keypoints = tf.convert_to_tensor(keypoints, dtype=tf.float32)\n",
        "\n",
        "    # Escala\n",
        "    if scale_range is not None:\n",
        "        scale_factor = tf.random.uniform([], scale_range[0], scale_range[1])\n",
        "        keypoints *= scale_factor\n",
        "\n",
        "    # Cisalhamento\n",
        "    if shear_range is not None:\n",
        "        xy = keypoints[..., :2]\n",
        "        z = keypoints[..., 2:]\n",
        "        shear_factor_x, shear_factor_y = tf.random.uniform([], shear_range[0], shear_range[1]),tf.random.uniform([], shear_range[0], shear_range[1])\n",
        "        if tf.random.uniform([]) < 0.5:\n",
        "            shear_factor_x = 0.\n",
        "        else:\n",
        "            shear_factor_y = 0.\n",
        "        shear_matrix = tf.convert_to_tensor([\n",
        "            [1, shear_factor_x],\n",
        "            [shear_factor_y, 1]\n",
        "        ], dtype=tf.float32)\n",
        "        xy = tf.matmul(xy, shear_matrix)\n",
        "        keypoints = tf.concat([xy, z], axis=-1)\n",
        "\n",
        "    # Rotação\n",
        "    if rotation_range is not None:\n",
        "        xy = keypoints[..., :2]\n",
        "        z = keypoints[..., 2:]\n",
        "        center = tf.constant([0.5, 0.5], dtype=tf.float32)\n",
        "        xy -= center\n",
        "        angle = tf.random.uniform([], rotation_range[0], rotation_range[1])\n",
        "        theta = angle * np.pi / 180  # Convertendo de graus para radianos\n",
        "        c = tf.cos(theta)\n",
        "        s = tf.sin(theta)\n",
        "        rotation_matrix = tf.convert_to_tensor([\n",
        "            [c, -s],\n",
        "            [s, c]\n",
        "        ], dtype=tf.float32)\n",
        "        xy = tf.matmul(xy, rotation_matrix)\n",
        "        xy += center\n",
        "        keypoints = tf.concat([xy, z], axis=-1)\n",
        "\n",
        "    # Translação\n",
        "    if translation_range is not None:\n",
        "        translation = tf.random.uniform([1, 3], translation_range[0], translation_range[1])\n",
        "        keypoints += translation\n",
        "\n",
        "    return keypoints\n",
        "\n",
        "@tf.function\n",
        "def interp1d(x, target_len, method='random'):\n",
        "    length = tf.shape(x)[1]\n",
        "    target_len = tf.maximum(1,target_len)\n",
        "    if method == 'random':\n",
        "        if tf.random.uniform(()) < 0.33:\n",
        "            x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bilinear')\n",
        "        else:\n",
        "            if tf.random.uniform(()) < 0.5:\n",
        "                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bicubic')\n",
        "            else:\n",
        "                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'nearest')\n",
        "    else:\n",
        "        x = tf.image.resize(x, (target_len,tf.shape(x)[1]),method)\n",
        "    return x\n",
        "\n",
        "@tf.function\n",
        "def resample(x, rate=(0.8,1.2)):\n",
        "  rate = tf.random.uniform((), rate[0], rate[1])\n",
        "  length = tf.shape(x)[0]\n",
        "  new_size = tf.cast(rate*tf.cast(length,tf.float32), tf.int32)\n",
        "  new_size = interp1d(x, new_size)\n",
        "  return new_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7O4MSQYxcMR"
      },
      "source": [
        "### Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOE8Ie3sxcMR"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def normalize(keypoints):\n",
        "    # Flatten the keypoints to 2D and then normalize\n",
        "    keypoints = tf.reshape(keypoints, [-1, keypoints.shape[1] * keypoints.shape[2]])\n",
        "    mean, variance = tf.nn.moments(keypoints, [-1], keepdims=True)\n",
        "    normalized_keypoints = tf.nn.batch_normalization(keypoints, mean, variance, offset=None, scale=None, variance_epsilon=1e-6)\n",
        "    return normalized_keypoints\n",
        "\n",
        "@tf.function\n",
        "def resize_pad(keypoints):\n",
        "    \"\"\"\n",
        "    Resize and pad the keypoints data to have a standard frame length.\n",
        "    \"\"\"\n",
        "    num_frames = tf.shape(keypoints)[0]\n",
        "\n",
        "    # Flatten the last two dimensions\n",
        "    keypoints = tf.reshape(keypoints, [num_frames, -1])\n",
        "\n",
        "    if num_frames < FRAME_LEN:\n",
        "        padding = [[0, FRAME_LEN - num_frames], [0, 0]]\n",
        "        keypoints = tf.pad(keypoints, padding)\n",
        "    else:\n",
        "        keypoints = keypoints[:FRAME_LEN, :]\n",
        "\n",
        "    return keypoints\n",
        "\n",
        "@tf.function\n",
        "def normalize(keypoints):\n",
        "    # Flatten the keypoints to 2D and then normalize\n",
        "    keypoints = tf.reshape(keypoints, [-1, keypoints.shape[1] * keypoints.shape[2]])\n",
        "    mean, variance = tf.nn.moments(keypoints, [-1], keepdims=True)\n",
        "    normalized_keypoints = tf.nn.batch_normalization(keypoints, mean, variance, offset=None, scale=None, variance_epsilon=1e-6)\n",
        "\n",
        "    return normalized_keypoints\n",
        "\n",
        "@tf.function\n",
        "def process_data(keypoints, caption):\n",
        "    keypoints = resize_pad(keypoints)\n",
        "    return keypoints, caption\n",
        "\n",
        "@tf.function\n",
        "def tf_load_and_normalize_keypoints_train(keypoints, caption):\n",
        "    keypoints = spatial_random_affine(keypoints)\n",
        "    #keypoints = resample(keypoints)\n",
        "    keypoints = normalize(keypoints)\n",
        "    return keypoints, caption\n",
        "\n",
        "@tf.function\n",
        "def tf_load_and_normalize_keypoints_non_train(keypoints, caption):\n",
        "    keypoints = normalize(keypoints)\n",
        "    return keypoints, caption\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fAva3XNVasq"
      },
      "outputs": [],
      "source": [
        "# Training pipeline with data augmentation\n",
        "train_dataset = (train_raw.map(tokenize_function)\n",
        "                          .map(tf_load_and_normalize_keypoints_train)\n",
        "                          .map(process_data)\n",
        "                          .batch(BATCH_SIZE))\n",
        "\n",
        "# Validation and test pipelines without data augmentation\n",
        "dev_dataset = (dev_raw.map(tokenize_function)\n",
        "                       .map(tf_load_and_normalize_keypoints_non_train)\n",
        "                       .map(process_data)\n",
        "                       .batch(BATCH_SIZE))\n",
        "\n",
        "test_dataset = (test_raw.map(tokenize_function)\n",
        "                        .map(tf_load_and_normalize_keypoints_non_train)\n",
        "                        .map(process_data)\n",
        "                        .batch(BATCH_SIZE))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhpG6eQKVf_i",
        "outputId": "857a087a-0efe-4eb2-9eb9-975c1e1e5f27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset:\n",
            "Keypoints Shape: (60, 180, 177)\n",
            "Captions Shape: (60, 30)\n",
            "\n",
            "Dev Dataset:\n",
            "Keypoints Shape: (60, 180, 177)\n",
            "Captions Shape: (60, 30)\n",
            "\n",
            "Test Dataset:\n",
            "Keypoints Shape: (60, 180, 177)\n",
            "Captions Shape: (60, 30)\n"
          ]
        }
      ],
      "source": [
        "def inspect_dataset_shape(dataset):\n",
        "    for keypoints, captions in dataset.take(1):\n",
        "        print(\"Keypoints Shape:\", keypoints.shape)\n",
        "        print(\"Captions Shape:\", captions.shape)\n",
        "        return keypoints.shape, captions.shape\n",
        "\n",
        "# Obtém o próximo lote de dados do conjunto de validação usando o iterador\n",
        "batch = next(iter(train_dataset))\n",
        "\n",
        "# Calcula a forma das entradas de dados no lote\n",
        "INPUT_SHAPE = batch[0].shape[1:]\n",
        "\n",
        "print(\"Train Dataset:\")\n",
        "train_shape = inspect_dataset_shape(train_dataset)\n",
        "print(\"\\nDev Dataset:\")\n",
        "dev_shape = inspect_dataset_shape(dev_dataset)\n",
        "print(\"\\nTest Dataset:\")\n",
        "test_shape = inspect_dataset_shape(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84uf4ImgxcMT"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7mupxb0xcMT"
      },
      "outputs": [],
      "source": [
        "class ECA(tf.keras.layers.Layer):\n",
        "    def __init__(self, kernel_size=5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.kernel_size = kernel_size\n",
        "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        \"\"\"\n",
        "        Realiza uma operação ECA (Enhanced Channel Attention) em tensores de entrada.\n",
        "\n",
        "        Args:\n",
        "            inputs (tf.Tensor): Tensor de entrada.\n",
        "            mask (tf.Tensor, opcional): Tensor de máscara para suportar sequências com comprimentos diferentes.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Tensor após a operação ECA.\n",
        "        \"\"\"\n",
        "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
        "        nn = tf.expand_dims(nn, -1)\n",
        "        nn = self.conv(nn)\n",
        "        nn = tf.squeeze(nn, -1)\n",
        "        nn = tf.nn.sigmoid(nn)\n",
        "        nn = nn[:, None, :]\n",
        "        return inputs * nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3_gk3uSxcMT"
      },
      "outputs": [],
      "source": [
        "class CausalDWConv1D(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 kernel_size=17,\n",
        "                 dilation_rate=1,\n",
        "                 use_bias=False,\n",
        "                 depthwise_initializer='glorot_uniform',\n",
        "                 name='',\n",
        "                 **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        # Adiciona uma camada de padding causal à esquerda dos dados de entrada.\n",
        "        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate * (kernel_size - 1), 0), name=name + '_pad')\n",
        "        # Aplica uma convolução depthwise causal à sequência de entrada.\n",
        "        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
        "            kernel_size,\n",
        "            strides=1,\n",
        "            dilation_rate=dilation_rate,\n",
        "            padding='valid',\n",
        "            use_bias=use_bias,\n",
        "            depthwise_initializer=depthwise_initializer,\n",
        "            name=name + '_dwconv')\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Realiza o padding causal à esquerda dos dados de entrada.\n",
        "        x = self.causal_pad(inputs)\n",
        "        # Aplica a convolução depthwise causal.\n",
        "        x = self.dw_conv(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "295_BRXnxcMU"
      },
      "outputs": [],
      "source": [
        "def Conv1DBlock(channel_size,\n",
        "                kernel_size,\n",
        "                dilation_rate=1,\n",
        "                drop_rate=0.0,\n",
        "                expand_ratio=2,\n",
        "                se_ratio=0.25,\n",
        "                activation='swish',\n",
        "                name=None):\n",
        "    '''\n",
        "    Efetua uma operação de bloco conv1d eficiente.\n",
        "\n",
        "    Args:\n",
        "        channel_size (int): Número de canais de saída.\n",
        "        kernel_size (int): Tamanho do kernel da convolução.\n",
        "        dilation_rate (int, opcional): Taxa de dilatação para convolução causal. Padrão é 1.\n",
        "        drop_rate (float, opcional): Taxa de dropout. Padrão é 0.0.\n",
        "        expand_ratio (int, opcional): Fator de expansão do canal. Padrão é 2.\n",
        "        se_ratio (float, opcional): Taxa de excitação espacial (SE). Padrão é 0.25.\n",
        "        activation (str, opcional): Função de ativação. Padrão é 'swish'.\n",
        "        name (str, opcional): Nome da camada. Padrão é None.\n",
        "\n",
        "    Returns:\n",
        "        Callable: Função que aplica o bloco conv1d eficiente.\n",
        "    '''\n",
        "    if name is None:\n",
        "        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n",
        "    # Fase de expansão\n",
        "    def apply(inputs):\n",
        "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
        "        channels_expand = channels_in * expand_ratio\n",
        "\n",
        "        skip = inputs\n",
        "\n",
        "        x = tf.keras.layers.Dense(\n",
        "            channels_expand,\n",
        "            use_bias=True,\n",
        "            activation=activation,\n",
        "            name=name + '_expand_conv')(inputs)\n",
        "\n",
        "        # Convolução Depthwise\n",
        "        x = CausalDWConv1D(kernel_size,\n",
        "                           dilation_rate=dilation_rate,\n",
        "                           use_bias=False,\n",
        "                           name=name + '_dwconv')(x)\n",
        "\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn')(x)\n",
        "\n",
        "        x = ECA()(x)\n",
        "\n",
        "        x = tf.keras.layers.Dense(\n",
        "            channel_size,\n",
        "            use_bias=True,\n",
        "            name=name + '_project_conv')(x)\n",
        "\n",
        "        if drop_rate > 0:\n",
        "            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + '_drop')(x)\n",
        "\n",
        "        if (channels_in == channel_size):\n",
        "            x = tf.keras.layers.add([x, skip], name=name + '_add')\n",
        "        return x\n",
        "\n",
        "    return apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayHTi7FpxcMU"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Inicialização da camada de atenção multi-head.\n",
        "        self.dim = dim\n",
        "        self.scale = self.dim ** -0.5\n",
        "        self.num_heads = num_heads\n",
        "        # Camada densa para calcular as consultas, chaves e valores da atenção.\n",
        "        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
        "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        # Calcula consultas, chaves e valores usando a camada densa.\n",
        "        qkv = self.qkv(inputs)\n",
        "        # Reorganiza os tensores para a forma necessária para atenção multi-head.\n",
        "        qkv = tf.keras.layers.Permute((2, 1, 3))(\n",
        "            tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv))\n",
        "        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n",
        "\n",
        "        # Calcula a atenção.\n",
        "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask[:, None, None, :]\n",
        "\n",
        "        # Aplica a função de Softmax à atenção, opcionalmente usando uma máscara.\n",
        "        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
        "        attn = self.drop1(attn)\n",
        "\n",
        "        # Calcula o resultado da atenção ponderada.\n",
        "        x = attn @ v\n",
        "        # Reorganiza o resultado de volta à forma original.\n",
        "        x = tf.keras.layers.Reshape((-1, self.dim))(\n",
        "            tf.keras.layers.Permute((2, 1, 3))(x))\n",
        "        # Projeta o resultado de volta à dimensão original.\n",
        "        x = self.proj(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxL69uBExcMV"
      },
      "outputs": [],
      "source": [
        "def TransformerBlock(dim=192, num_heads=6, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish'):\n",
        "    \"\"\"\n",
        "    Bloco de Transformer personalizado.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Dimensão do espaço de características.\n",
        "        num_heads (int): Número de cabeças de atenção multi-head.\n",
        "        expand (int): Fator de expansão para a camada densa interna.\n",
        "        attn_dropout (float): Taxa de dropout para a camada de atenção multi-head.\n",
        "        drop_rate (float): Taxa de dropout para as camadas de dropout.\n",
        "        activation (str): Função de ativação para as camadas densas internas.\n",
        "\n",
        "    Returns:\n",
        "        Callable: Função que aplica o bloco de Transformer a um tensor de entrada.\n",
        "    \"\"\"\n",
        "    def apply(inputs):\n",
        "        # Ajusta a dimensão de entrada para `dim`.\n",
        "        reshaped_inputs = tf.keras.layers.Dense(dim, use_bias=False)(inputs)\n",
        "\n",
        "        x = reshaped_inputs\n",
        "        # Normalização por camada antes da camada de atenção.\n",
        "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        # Camada de atenção multi-head.\n",
        "        x = MultiHeadSelfAttention(dim=dim, num_heads=num_heads, dropout=attn_dropout)(x)\n",
        "        # Camada de dropout após a atenção.\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n",
        "        # Adição da saída da camada de atenção à entrada ajustada.\n",
        "        x = tf.keras.layers.Add()([reshaped_inputs, x])\n",
        "        attn_out = x  # Saída da camada de atenção.\n",
        "\n",
        "        # Normalização por camada antes das camadas densas internas.\n",
        "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        # Primeira camada densa interna.\n",
        "        x = tf.keras.layers.Dense(dim * expand, use_bias=False, activation=activation)(x)\n",
        "        # Segunda camada densa interna.\n",
        "        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n",
        "        # Camada de dropout após as camadas densas internas.\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n",
        "        # Adição da saída das camadas densas internas à saída da camada de atenção.\n",
        "        x = tf.keras.layers.Add()([attn_out, x])\n",
        "        return x\n",
        "\n",
        "    return apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4sqhBAXxcMV"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(maxlen, num_hid):\n",
        "    \"\"\"\n",
        "    Gera a codificação posicional para sequências de entrada.\n",
        "\n",
        "    Args:\n",
        "        maxlen (int): Comprimento máximo da sequência.\n",
        "        num_hid (int): Número de dimensões ocultas para a codificação posicional.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Codifica ção posicional para a sequência de entrada.\n",
        "    \"\"\"\n",
        "    depth = num_hid / 2\n",
        "    positions = tf.range(maxlen, dtype=tf.float32)[..., tf.newaxis]\n",
        "    depths = tf.range(depth, dtype=tf.float32)[np.newaxis, :] / depth\n",
        "    angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n",
        "    angle_rads = tf.linalg.matmul(positions, angle_rates)\n",
        "\n",
        "    # Calcula as funções trigonométricas para a codificação posicional.\n",
        "    sin_vals = tf.math.sin(angle_rads)\n",
        "    cos_vals = tf.math.cos(angle_rads)\n",
        "\n",
        "    # Concatena as funções seno e cosseno para formar a codificação posicional.\n",
        "    pos_encoding = tf.concat([sin_vals, cos_vals], axis=-1)\n",
        "    return pos_encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx9h1pFEtYVi"
      },
      "outputs": [],
      "source": [
        "def CTCLoss(labels, logits):\n",
        "    \"\"\"\n",
        "    Calcula a perda CTC (Connectionist Temporal Classification).\n",
        "\n",
        "    Args:\n",
        "        labels (tf.Tensor): Rótulos verdadeiros da sequência.\n",
        "        logits (tf.Tensor): Logits produzidos pelo modelo.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Valor da perda CTC.\n",
        "    \"\"\"\n",
        "    # Certifique-se de que os rótulos estão no formato int32\n",
        "    #labels = tf.cast(labels, tf.int32)\n",
        "\n",
        "    # Certifique-se de que os logits estão no formato float32\n",
        "    #logits = tf.cast(logits, tf.float32)\n",
        "\n",
        "    # Calcula o comprimento real dos rótulos removendo os tokens de preenchimento.\n",
        "    label_length = tf.reduce_sum(tf.cast(labels != pad_token_idx, tf.int32), axis=-1)\n",
        "\n",
        "    # Calcula o comprimento dos logits como o comprimento máximo da sequência de saída.\n",
        "    logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n",
        "\n",
        "    # Calcula a perda CTC.\n",
        "    loss = tf.nn.ctc_loss(\n",
        "        labels=labels,\n",
        "        logits=logits,\n",
        "        label_length=label_length,\n",
        "        logit_length=logit_length,\n",
        "        blank_index=pad_token_idx,\n",
        "        logits_time_major=False  # Os logits não estão no formato \"tempo primeiro\".\n",
        "    )\n",
        "\n",
        "    # Calcula a média da perda.\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxHOfIjBxcMW"
      },
      "outputs": [],
      "source": [
        "def get_model(dim=384, vocab_size=tokenizers.en.get_vocab_size().numpy()):\n",
        "    \"\"\"\n",
        "    Cria e retorna um modelo de rede neural.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Dimensão das camadas do modelo.\n",
        "        vocab_size (int): Tamanho do vocabulário.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: O modelo de rede neural criado.\n",
        "    \"\"\"\n",
        "    inp = tf.keras.Input(INPUT_SHAPE)\n",
        "    x = tf.keras.layers.Masking(mask_value=0.0)(inp)  # Máscara para ignorar sequências de preenchimento.\n",
        "    x = tf.keras.layers.Dense(dim, use_bias=False, name='stem_conv')(x)  # Camada densa inicial.\n",
        "    pe = tf.cast(positional_encoding(INPUT_SHAPE[0], dim), dtype=x.dtype)  # Codificação posicional.\n",
        "    x = x + pe  # Adiciona a codificação posicional às entradas.\n",
        "    x = tf.keras.layers.BatchNormalization(momentum=0.95, name='stem_bn')(x)  # Normalização em lote.\n",
        "\n",
        "    num_blocks = 6  # Número de blocos no modelo.\n",
        "    drop_rate = 0.2  # Taxa de dropout.\n",
        "\n",
        "    for i in range(num_blocks):\n",
        "        # Camadas de bloco convolucional.\n",
        "        x = Conv1DBlock(dim, 11, drop_rate=drop_rate)(x)\n",
        "        x = Conv1DBlock(dim, 5, drop_rate=drop_rate)(x)\n",
        "        #x = Conv1DBlock(dim, 3, drop_rate=drop_rate)(x)\n",
        "        # Bloco de transformador.\n",
        "        x = TransformerBlock(dim, expand=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(dim * 2, activation='relu', name='top_conv')(x)  # Camada densa intermediária.\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)  # Dropout.\n",
        "    x = tf.keras.layers.Dense(vocab_size, name='classifier')(x)  # Camada de classificação.\n",
        "\n",
        "    model = tf.keras.Model(inp, x)  # Cria o modelo.\n",
        "\n",
        "    loss = CTCLoss  # Função de perda CTC.\n",
        "\n",
        "    # Otimizador Adam com retificação e lookahead.\n",
        "    optimizer = tf.optimizers.AdamW(learning_rate=4e-4, weight_decay=1e-4)\n",
        "\n",
        "    model.compile(loss=loss, optimizer=optimizer)  # Compila o modelo.\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE2dlUADxcMX",
        "outputId": "40bf7ac5-32dc-40ee-9429-a9a99ed18a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 180, 177)]           0         []                            \n",
            "                                                                                                  \n",
            " masking (Masking)           (None, 180, 177)             0         ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " stem_conv (Dense)           (None, 180, 384)             67968     ['masking[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOp  (None, 180, 384)             0         ['stem_conv[0][0]']           \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " stem_bn (BatchNormalizatio  (None, 180, 384)             1536      ['tf.__operators__.add[0][0]']\n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " 1_expand_conv (Dense)       (None, 180, 768)             295680    ['stem_bn[0][0]']             \n",
            "                                                                                                  \n",
            " 1_dwconv (CausalDWConv1D)   (None, 180, 768)             8448      ['1_expand_conv[0][0]']       \n",
            "                                                                                                  \n",
            " 1_bn (BatchNormalization)   (None, 180, 768)             3072      ['1_dwconv[0][0]']            \n",
            "                                                                                                  \n",
            " eca (ECA)                   (None, 180, 768)             5         ['1_bn[0][0]']                \n",
            "                                                                                                  \n",
            " 1_project_conv (Dense)      (None, 180, 384)             295296    ['eca[0][0]']                 \n",
            "                                                                                                  \n",
            " 1_drop (Dropout)            (None, 180, 384)             0         ['1_project_conv[0][0]']      \n",
            "                                                                                                  \n",
            " 1_add (Add)                 (None, 180, 384)             0         ['1_drop[0][0]',              \n",
            "                                                                     'stem_bn[0][0]']             \n",
            "                                                                                                  \n",
            " 2_expand_conv (Dense)       (None, 180, 768)             295680    ['1_add[0][0]']               \n",
            "                                                                                                  \n",
            " 2_dwconv (CausalDWConv1D)   (None, 180, 768)             3840      ['2_expand_conv[0][0]']       \n",
            "                                                                                                  \n",
            " 2_bn (BatchNormalization)   (None, 180, 768)             3072      ['2_dwconv[0][0]']            \n",
            "                                                                                                  \n",
            " eca_1 (ECA)                 (None, 180, 768)             5         ['2_bn[0][0]']                \n",
            "                                                                                                  \n",
            " 2_project_conv (Dense)      (None, 180, 384)             295296    ['eca_1[0][0]']               \n",
            "                                                                                                  \n",
            " 2_drop (Dropout)            (None, 180, 384)             0         ['2_project_conv[0][0]']      \n",
            "                                                                                                  \n",
            " 2_add (Add)                 (None, 180, 384)             0         ['2_drop[0][0]',              \n",
            "                                                                     '1_add[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 180, 384)             147456    ['2_add[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 180, 384)             768       ['dense[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_self_attention   (None, 180, 384)             589824    ['layer_normalization[0][0]'] \n",
            " (MultiHeadSelfAttention)                                                                         \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 180, 384)             0         ['multi_head_self_attention[0]\n",
            "                                                                    [0]']                         \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 180, 384)             0         ['dense[0][0]',               \n",
            "                                                                     'dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 180, 384)             768       ['add[0][0]']                 \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 180, 768)             294912    ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 180, 384)             294912    ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 180, 384)             0         ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 180, 384)             0         ['add[0][0]',                 \n",
            "                                                                     'dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " 3_expand_conv (Dense)       (None, 180, 768)             295680    ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " 3_dwconv (CausalDWConv1D)   (None, 180, 768)             8448      ['3_expand_conv[0][0]']       \n",
            "                                                                                                  \n",
            " 3_bn (BatchNormalization)   (None, 180, 768)             3072      ['3_dwconv[0][0]']            \n",
            "                                                                                                  \n",
            " eca_2 (ECA)                 (None, 180, 768)             5         ['3_bn[0][0]']                \n",
            "                                                                                                  \n",
            " 3_project_conv (Dense)      (None, 180, 384)             295296    ['eca_2[0][0]']               \n",
            "                                                                                                  \n",
            " 3_drop (Dropout)            (None, 180, 384)             0         ['3_project_conv[0][0]']      \n",
            "                                                                                                  \n",
            " 3_add (Add)                 (None, 180, 384)             0         ['3_drop[0][0]',              \n",
            "                                                                     'add_1[0][0]']               \n",
            "                                                                                                  \n",
            " 4_expand_conv (Dense)       (None, 180, 768)             295680    ['3_add[0][0]']               \n",
            "                                                                                                  \n",
            " 4_dwconv (CausalDWConv1D)   (None, 180, 768)             3840      ['4_expand_conv[0][0]']       \n",
            "                                                                                                  \n",
            " 4_bn (BatchNormalization)   (None, 180, 768)             3072      ['4_dwconv[0][0]']            \n",
            "                                                                                                  \n",
            " eca_3 (ECA)                 (None, 180, 768)             5         ['4_bn[0][0]']                \n",
            "                                                                                                  \n",
            " 4_project_conv (Dense)      (None, 180, 384)             295296    ['eca_3[0][0]']               \n",
            "                                                                                                  \n",
            " 4_drop (Dropout)            (None, 180, 384)             0         ['4_project_conv[0][0]']      \n",
            "                                                                                                  \n",
            " 4_add (Add)                 (None, 180, 384)             0         ['4_drop[0][0]',              \n",
            "                                                                     '3_add[0][0]']               \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 180, 384)             147456    ['4_add[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 180, 384)             768       ['dense_5[0][0]']             \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_self_attention_  (None, 180, 384)             589824    ['layer_normalization_2[0][0]'\n",
            " 1 (MultiHeadSelfAttention)                                         ]                             \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 180, 384)             0         ['multi_head_self_attention_1[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 180, 384)             0         ['dense_5[0][0]',             \n",
            "                                                                     'dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 180, 384)             768       ['add_2[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 180, 768)             294912    ['layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 180, 384)             294912    ['dense_8[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 180, 384)             0         ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 180, 384)             0         ['add_2[0][0]',               \n",
            "                                                                     'dropout_5[0][0]']           \n",
            "                                                                                                  \n",
            " 5_expand_conv (Dense)       (None, 180, 768)             295680    ['add_3[0][0]']               \n",
            "                                                                                                  \n",
            " 5_dwconv (CausalDWConv1D)   (None, 180, 768)             8448      ['5_expand_conv[0][0]']       \n",
            "                                                                                                  \n",
            " 5_bn (BatchNormalization)   (None, 180, 768)             3072      ['5_dwconv[0][0]']            \n",
            "                                                                                                  \n",
            " eca_4 (ECA)                 (None, 180, 768)             5         ['5_bn[0][0]']                \n",
            "                                                                                                  \n",
            " 5_project_conv (Dense)      (None, 180, 384)             295296    ['eca_4[0][0]']               \n",
            "                                                                                                  \n",
            " 5_drop (Dropout)            (None, 180, 384)             0         ['5_project_conv[0][0]']      \n",
            "                                                                                                  \n",
            " 5_add (Add)                 (None, 180, 384)             0         ['5_drop[0][0]',              \n",
            "                                                                     'add_3[0][0]']               \n",
            "                                                                                                  \n",
            " 6_expand_conv (Dense)       (None, 180, 768)             295680    ['5_add[0][0]']               \n",
            "                                                                                                  \n",
            " 6_dwconv (CausalDWConv1D)   (None, 180, 768)             3840      ['6_expand_conv[0][0]']       \n",
            "                                                                                                  \n",
            " 6_bn (BatchNormalization)   (None, 180, 768)             3072      ['6_dwconv[0][0]']            \n",
            "                                                                                                  \n",
            " eca_5 (ECA)                 (None, 180, 768)             5         ['6_bn[0][0]']                \n",
            "                                                                                                  \n",
            " 6_project_conv (Dense)      (None, 180, 384)             295296    ['eca_5[0][0]']               \n",
            "                                                                                                  \n",
            " 6_drop (Dropout)            (None, 180, 384)             0         ['6_project_conv[0][0]']      \n",
            "                                                                                                  \n",
            " 6_add (Add)                 (None, 180, 384)             0         ['6_drop[0][0]',              \n",
            "                                                                     '5_add[0][0]']               \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 180, 384)             147456    ['6_add[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 180, 384)             768       ['dense_10[0][0]']            \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_self_attention_  (None, 180, 384)             589824    ['layer_normalization_4[0][0]'\n",
            " 2 (MultiHeadSelfAttention)                                         ]                             \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)         (None, 180, 384)             0         ['multi_head_self_attention_2[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 180, 384)             0         ['dense_10[0][0]',            \n",
            "                                                                     'dropout_7[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 180, 384)             768       ['add_4[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 180, 768)             294912    ['layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 180, 384)             294912    ['dense_13[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)         (None, 180, 384)             0         ['dense_14[0][0]']            \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 180, 384)             0         ['add_4[0][0]',               \n",
            "                                                                     'dropout_8[0][0]']           \n",
            "                                                                                                  \n",
            " 7_expand_conv (Dense)       (None, 180, 768)             295680    ['add_5[0][0]']               \n",
            "                                                                                                  \n",
            " 7_dwconv (CausalDWConv1D)   (None, 180, 768)             8448      ['7_expand_conv[0][0]']       \n",
            "                                                                                                  \n",
            " 7_bn (BatchNormalization)   (None, 180, 768)             3072      ['7_dwconv[0][0]']            \n",
            "                                                                                                  \n",
            " eca_6 (ECA)                 (None, 180, 768)             5         ['7_bn[0][0]']                \n",
            "                                                                                                  \n",
            " 7_project_conv (Dense)      (None, 180, 384)             295296    ['eca_6[0][0]']               \n",
            "                                                                                                  \n",
            " 7_drop (Dropout)            (None, 180, 384)             0         ['7_project_conv[0][0]']      \n",
            "                                                                                                  \n",
            " 7_add (Add)                 (None, 180, 384)             0         ['7_drop[0][0]',              \n",
            "                                                                     'add_5[0][0]']               \n",
            "                                                                                                  \n",
            " 8_expand_conv (Dense)       (None, 180, 768)             295680    ['7_add[0][0]']               \n",
            "                                                                                                  \n",
            " 8_dwconv (CausalDWConv1D)   (None, 180, 768)             3840      ['8_expand_conv[0][0]']       \n",
            "                                                                                                  \n",
            " 8_bn (BatchNormalization)   (None, 180, 768)             3072      ['8_dwconv[0][0]']            \n",
            "                                                                                                  \n",
            " eca_7 (ECA)                 (None, 180, 768)             5         ['8_bn[0][0]']                \n",
            "                                                                                                  \n",
            " 8_project_conv (Dense)      (None, 180, 384)             295296    ['eca_7[0][0]']               \n",
            "                                                                                                  \n",
            " 8_drop (Dropout)            (None, 180, 384)             0         ['8_project_conv[0][0]']      \n",
            "                                                                                                  \n",
            " 8_add (Add)                 (None, 180, 384)             0         ['8_drop[0][0]',              \n",
            "                                                                     '7_add[0][0]']               \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 180, 384)             147456    ['8_add[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_6 (Lay  (None, 180, 384)             768       ['dense_15[0][0]']            \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_self_attention_  (None, 180, 384)             589824    ['layer_normalization_6[0][0]'\n",
            " 3 (MultiHeadSelfAttention)                                         ]                             \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)        (None, 180, 384)             0         ['multi_head_self_attention_3[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 180, 384)             0         ['dense_15[0][0]',            \n",
            "                                                                     'dropout_10[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_7 (Lay  (None, 180, 384)             768       ['add_6[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_18 (Dense)            (None, 180, 768)             294912    ['layer_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_19 (Dense)            (None, 180, 384)             294912    ['dense_18[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)        (None, 180, 384)             0         ['dense_19[0][0]']            \n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 180, 384)             0         ['add_6[0][0]',               \n",
            "                                                                     'dropout_11[0][0]']          \n",
            "                                                                                                  \n",
            " 9_expand_conv (Dense)       (None, 180, 768)             295680    ['add_7[0][0]']               \n",
            "                                                                                                  \n",
            " 9_dwconv (CausalDWConv1D)   (None, 180, 768)             8448      ['9_expand_conv[0][0]']       \n",
            "                                                                                                  \n",
            " 9_bn (BatchNormalization)   (None, 180, 768)             3072      ['9_dwconv[0][0]']            \n",
            "                                                                                                  \n",
            " eca_8 (ECA)                 (None, 180, 768)             5         ['9_bn[0][0]']                \n",
            "                                                                                                  \n",
            " 9_project_conv (Dense)      (None, 180, 384)             295296    ['eca_8[0][0]']               \n",
            "                                                                                                  \n",
            " 9_drop (Dropout)            (None, 180, 384)             0         ['9_project_conv[0][0]']      \n",
            "                                                                                                  \n",
            " 9_add (Add)                 (None, 180, 384)             0         ['9_drop[0][0]',              \n",
            "                                                                     'add_7[0][0]']               \n",
            "                                                                                                  \n",
            " 10_expand_conv (Dense)      (None, 180, 768)             295680    ['9_add[0][0]']               \n",
            "                                                                                                  \n",
            " 10_dwconv (CausalDWConv1D)  (None, 180, 768)             3840      ['10_expand_conv[0][0]']      \n",
            "                                                                                                  \n",
            " 10_bn (BatchNormalization)  (None, 180, 768)             3072      ['10_dwconv[0][0]']           \n",
            "                                                                                                  \n",
            " eca_9 (ECA)                 (None, 180, 768)             5         ['10_bn[0][0]']               \n",
            "                                                                                                  \n",
            " 10_project_conv (Dense)     (None, 180, 384)             295296    ['eca_9[0][0]']               \n",
            "                                                                                                  \n",
            " 10_drop (Dropout)           (None, 180, 384)             0         ['10_project_conv[0][0]']     \n",
            "                                                                                                  \n",
            " 10_add (Add)                (None, 180, 384)             0         ['10_drop[0][0]',             \n",
            "                                                                     '9_add[0][0]']               \n",
            "                                                                                                  \n",
            " dense_20 (Dense)            (None, 180, 384)             147456    ['10_add[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_8 (Lay  (None, 180, 384)             768       ['dense_20[0][0]']            \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " multi_head_self_attention_  (None, 180, 384)             589824    ['layer_normalization_8[0][0]'\n",
            " 4 (MultiHeadSelfAttention)                                         ]                             \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)        (None, 180, 384)             0         ['multi_head_self_attention_4[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " add_8 (Add)                 (None, 180, 384)             0         ['dense_20[0][0]',            \n",
            "                                                                     'dropout_13[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_9 (Lay  (None, 180, 384)             768       ['add_8[0][0]']               \n",
            " erNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dense_23 (Dense)            (None, 180, 768)             294912    ['layer_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_24 (Dense)            (None, 180, 384)             294912    ['dense_23[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)        (None, 180, 384)             0         ['dense_24[0][0]']            \n",
            "                                                                                                  \n",
            " add_9 (Add)                 (None, 180, 384)             0         ['add_8[0][0]',               \n",
            "                                                                     'dropout_14[0][0]']          \n",
            "                                                                                                  \n",
            " 11_expand_conv (Dense)      (None, 180, 768)             295680    ['add_9[0][0]']               \n",
            "                                                                                                  \n",
            " 11_dwconv (CausalDWConv1D)  (None, 180, 768)             8448      ['11_expand_conv[0][0]']      \n",
            "                                                                                                  \n",
            " 11_bn (BatchNormalization)  (None, 180, 768)             3072      ['11_dwconv[0][0]']           \n",
            "                                                                                                  \n",
            " eca_10 (ECA)                (None, 180, 768)             5         ['11_bn[0][0]']               \n",
            "                                                                                                  \n",
            " 11_project_conv (Dense)     (None, 180, 384)             295296    ['eca_10[0][0]']              \n",
            "                                                                                                  \n",
            " 11_drop (Dropout)           (None, 180, 384)             0         ['11_project_conv[0][0]']     \n",
            "                                                                                                  \n",
            " 11_add (Add)                (None, 180, 384)             0         ['11_drop[0][0]',             \n",
            "                                                                     'add_9[0][0]']               \n",
            "                                                                                                  \n",
            " 12_expand_conv (Dense)      (None, 180, 768)             295680    ['11_add[0][0]']              \n",
            "                                                                                                  \n",
            " 12_dwconv (CausalDWConv1D)  (None, 180, 768)             3840      ['12_expand_conv[0][0]']      \n",
            "                                                                                                  \n",
            " 12_bn (BatchNormalization)  (None, 180, 768)             3072      ['12_dwconv[0][0]']           \n",
            "                                                                                                  \n",
            " eca_11 (ECA)                (None, 180, 768)             5         ['12_bn[0][0]']               \n",
            "                                                                                                  \n",
            " 12_project_conv (Dense)     (None, 180, 384)             295296    ['eca_11[0][0]']              \n",
            "                                                                                                  \n",
            " 12_drop (Dropout)           (None, 180, 384)             0         ['12_project_conv[0][0]']     \n",
            "                                                                                                  \n",
            " 12_add (Add)                (None, 180, 384)             0         ['12_drop[0][0]',             \n",
            "                                                                     '11_add[0][0]']              \n",
            "                                                                                                  \n",
            " dense_25 (Dense)            (None, 180, 384)             147456    ['12_add[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_10 (La  (None, 180, 384)             768       ['dense_25[0][0]']            \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " multi_head_self_attention_  (None, 180, 384)             589824    ['layer_normalization_10[0][0]\n",
            " 5 (MultiHeadSelfAttention)                                         ']                            \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)        (None, 180, 384)             0         ['multi_head_self_attention_5[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " add_10 (Add)                (None, 180, 384)             0         ['dense_25[0][0]',            \n",
            "                                                                     'dropout_16[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_11 (La  (None, 180, 384)             768       ['add_10[0][0]']              \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " dense_28 (Dense)            (None, 180, 768)             294912    ['layer_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_29 (Dense)            (None, 180, 384)             294912    ['dense_28[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)        (None, 180, 384)             0         ['dense_29[0][0]']            \n",
            "                                                                                                  \n",
            " add_11 (Add)                (None, 180, 384)             0         ['add_10[0][0]',              \n",
            "                                                                     'dropout_17[0][0]']          \n",
            "                                                                                                  \n",
            " top_conv (Dense)            (None, 180, 768)             295680    ['add_11[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)        (None, 180, 768)             0         ['top_conv[0][0]']            \n",
            "                                                                                                  \n",
            " classifier (Dense)          (None, 180, 4446)            3418974   ['dropout_18[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 18958362 (72.32 MB)\n",
            "Trainable params: 18939162 (72.25 MB)\n",
            "Non-trainable params: 19200 (75.00 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = get_model()\n",
        "model(batch[0])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0kiojalBEHI"
      },
      "outputs": [],
      "source": [
        "def decode_phrase(pred):\n",
        "    \"\"\"\n",
        "    Decodifica uma previsão da rede neural em uma sequência de tokens.\n",
        "\n",
        "    Args:\n",
        "        pred (tf.Tensor): Tensor contendo as previsões da rede neural.\n",
        "        tokenizer (tf.keras.layers.TextVectorization): Tokenizador.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: Lista de tokens decodificados.\n",
        "    \"\"\"\n",
        "    # Obter o índice do token com a maior probabilidade para cada posição\n",
        "    x = tf.argmax(pred, axis=1)\n",
        "    # Remover índices consecutivos idênticos\n",
        "    diff = tf.not_equal(x[:-1], x[1:])\n",
        "    adjacent_indices = tf.where(diff)[:, 0]\n",
        "    x = tf.gather(x, adjacent_indices)\n",
        "    # Remover o índice do token de padding\n",
        "    mask = x != pad_token_idx\n",
        "    x = tf.boolean_mask(x, mask, axis=0)\n",
        "    return x\n",
        "\n",
        "def decode_batch_predictions(pred, tokenizer):\n",
        "    \"\"\"\n",
        "    Decodifica as previsões da rede neural em uma lista de frases.\n",
        "\n",
        "    Args:\n",
        "        pred (List[tf.Tensor]): Lista de tensores contendo as previsões da rede neural.\n",
        "        tokenizer (Tokenizer): O tokenizador.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: Lista de frases decodificadas.\n",
        "    \"\"\"\n",
        "    output_text = []\n",
        "    for result in pred:\n",
        "        tokens = decode_phrase(result).numpy()\n",
        "        sentence = tokenizer.detokenize([tokens])\n",
        "        sentence = sentence.numpy()[0].decode('utf-8')\n",
        "        output_text.append(sentence)\n",
        "    return output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl5yGoFZxcMY"
      },
      "outputs": [],
      "source": [
        "class CallbackEval(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Uma classe de callback para exibir algumas transcrições durante o treinamento.\n",
        "\n",
        "    Args:\n",
        "        dataset (tf.data.Dataset): O conjunto de dados de validação para avaliar as transcrições.\n",
        "        tokenizer (Tokenizer): O tokenizador.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs=None):\n",
        "        \"\"\"\n",
        "        Método chamado no final de cada época durante o treinamento.\n",
        "\n",
        "        Args:\n",
        "            epoch (int): O número da época atual.\n",
        "            logs: Dicionário contendo as métricas de treinamento.\n",
        "        \"\"\"\n",
        "        predictions = []  # Armazena as transcrições previstas\n",
        "        targets = []  # Armazena as transcrições reais\n",
        "\n",
        "        # Loop através dos lotes do conjunto de dados de validação\n",
        "        for batch in self.dataset:\n",
        "            X, y = batch\n",
        "            batch_predictions = self.model(X)  # Obtém previsões para o lote\n",
        "            batch_predictions = decode_batch_predictions(batch_predictions, self.tokenizer)  # Decodifica as previsões\n",
        "            predictions.extend(batch_predictions)  # Adiciona as previsões à lista\n",
        "            for label in y:\n",
        "                # Converte os rótulos em sequências de palavras\n",
        "                decoded_label = self.tokenizer.detokenize([label])\n",
        "                targets.append(decoded_label.numpy()[0].decode('utf-8'))  # Adiciona os rótulos à lista de metas\n",
        "\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # Exibe transcrições de um subconjunto aleatório de exemplos (32 no total)\n",
        "        for i in range(15):\n",
        "            print(f\"Target    : {targets[i]}\")\n",
        "            print(f\"Prediction: {predictions[i]}, len: {len(predictions[i].split())}\")\n",
        "            print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjn6PZCvUCid"
      },
      "source": [
        "## Trainning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Unq_gNo0xcMd",
        "outputId": "8fa564ee-35a8-4d82-bb26-afde693ed9ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "503/503 [==============================] - 668s 1s/step - loss: 112.5435 - val_loss: 101.8278\n",
            "Epoch 2/120\n",
            "503/503 [==============================] - 636s 1s/step - loss: 101.4679 - val_loss: 99.8776\n",
            "Epoch 3/120\n",
            "503/503 [==============================] - 635s 1s/step - loss: 99.6741 - val_loss: 98.6238\n",
            "Epoch 4/120\n",
            "503/503 [==============================] - 634s 1s/step - loss: 98.6329 - val_loss: 98.9664\n",
            "Epoch 5/120\n",
            "503/503 [==============================] - 635s 1s/step - loss: 97.6288 - val_loss: 98.9904\n",
            "Epoch 6/120\n",
            "503/503 [==============================] - 634s 1s/step - loss: 96.7596 - val_loss: 98.2000\n",
            "Epoch 7/120\n",
            "503/503 [==============================] - 633s 1s/step - loss: 96.0397 - val_loss: 98.0277\n",
            "Epoch 8/120\n",
            "503/503 [==============================] - 634s 1s/step - loss: 95.2632 - val_loss: 98.8428\n",
            "Epoch 9/120\n",
            "503/503 [==============================] - 634s 1s/step - loss: 94.5513 - val_loss: 97.8012\n",
            "Epoch 10/120\n",
            "503/503 [==============================] - 635s 1s/step - loss: 94.0494 - val_loss: 97.1391\n",
            "Epoch 11/120\n",
            "503/503 [==============================] - 634s 1s/step - loss: 93.6971 - val_loss: 96.5827\n",
            "Epoch 12/120\n",
            "503/503 [==============================] - 634s 1s/step - loss: 93.2148 - val_loss: 96.1673\n",
            "Epoch 13/120\n",
            "503/503 [==============================] - 634s 1s/step - loss: 92.8485 - val_loss: 96.7606\n",
            "Epoch 14/120\n",
            "503/503 [==============================] - 635s 1s/step - loss: 92.4853 - val_loss: 96.6896\n",
            "Epoch 15/120\n",
            "503/503 [==============================] - 639s 1s/step - loss: 92.3271 - val_loss: 96.4261\n",
            "Epoch 16/120\n",
            "503/503 [==============================] - 640s 1s/step - loss: 92.2032 - val_loss: 97.0927\n",
            "Epoch 17/120\n",
            "503/503 [==============================] - 643s 1s/step - loss: 92.1382 - val_loss: 98.4363\n",
            "Epoch 18/120\n",
            "503/503 [==============================] - 644s 1s/step - loss: 91.7895 - val_loss: 100.6211\n",
            "Epoch 19/120\n",
            "503/503 [==============================] - 639s 1s/step - loss: 91.5130 - val_loss: 105.1817\n",
            "Epoch 20/120\n",
            "503/503 [==============================] - 637s 1s/step - loss: 91.3728 - val_loss: 102.2867\n",
            "Epoch 21/120\n",
            "503/503 [==============================] - 633s 1s/step - loss: 91.0554 - val_loss: 106.3293\n",
            "Epoch 22/120\n",
            "503/503 [==============================] - 633s 1s/step - loss: 90.7596 - val_loss: 107.5272\n",
            "Epoch 23/120\n",
            "503/503 [==============================] - 641s 1s/step - loss: 90.7724 - val_loss: 104.1550\n",
            "Epoch 24/120\n",
            "503/503 [==============================] - 642s 1s/step - loss: 90.5718 - val_loss: 106.2719\n",
            "Epoch 25/120\n",
            "503/503 [==============================] - 639s 1s/step - loss: 90.2220 - val_loss: 105.9048\n",
            "Epoch 26/120\n",
            "503/503 [==============================] - 637s 1s/step - loss: 95.9269 - val_loss: 101.9588\n",
            "Epoch 27/120\n",
            "503/503 [==============================] - 638s 1s/step - loss: 91.4311 - val_loss: 100.9322\n",
            "Epoch 28/120\n",
            "503/503 [==============================] - 639s 1s/step - loss: 90.2755 - val_loss: 101.9493\n",
            "Epoch 29/120\n",
            "503/503 [==============================] - 642s 1s/step - loss: 89.5606 - val_loss: 100.7723\n",
            "Epoch 30/120\n",
            "456/503 [==========================>...] - ETA: 58s - loss: 88.8184 "
          ]
        }
      ],
      "source": [
        "# Função de callback para verificar a transcrição no conjunto de validação.\n",
        "save_dir = '/content/gdrive/MyDrive/h2s'\n",
        "#validation_callback = CallbackEval(dataset, tokenizer)\n",
        "\n",
        "# Lista existente de callbacks\n",
        "#callbacks_list = [\n",
        "#    validation_callback,\n",
        "#    wandb.keras.WandbCallback(save_model=True, monitor='val_loss', mode='min')  # Salvar o melhor modelo com base na perda de validação\n",
        "#]\n",
        "\n",
        "# Treinamento do modelo\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=dev_dataset,\n",
        "    epochs=N_EPOCHS,\n",
        "    #callbacks=callbacks_list\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9CQ-JpRdxa5"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "# Get the current timestamp\n",
        "current_time = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "model_filename = f'H2S_{current_time}.keras'\n",
        "\n",
        "# Save the model\n",
        "model.save(f'{save_dir}/{model_filename}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRPGkMXQxcMd"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['training loss', 'val_loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NZMGuGUxcMe"
      },
      "outputs": [],
      "source": [
        "# Fazer previsões no conjunto de dados de validação (dev_dataset)\n",
        "predictions_dev  = []  # Armazena as transcrições previstas\n",
        "targets_dev = []  # Armazena as transcrições reais\n",
        "\n",
        "# Loop através dos lotes do conjunto de dados de validação\n",
        "for batch in dev_dataset:\n",
        "    X, y = batch\n",
        "    batch_predictions = model(X)  # Obtém previsões para o lote\n",
        "    batch_predictions = decode_batch_predictions(batch_predictions, tokenizer)  # Decodifica as previsões\n",
        "    predictions_dev.extend(batch_predictions)  # Adiciona as previsões à lista\n",
        "    for label in y:\n",
        "        # Converte os rótulos em sequências de palavras\n",
        "        decoded_label = tokenizer.detokenize([label]).numpy()[0].decode('utf-8')\n",
        "        targets_dev.append(decoded_label)  # Adiciona os rótulos à lista de metas\n",
        "\n",
        "# Exibe transcrições previstas e rótulos reais\n",
        "for i in range(len(predictions_dev)):\n",
        "    print(f\"Target    : {targets_dev[i]}\")\n",
        "    print(f\"Prediction: {predictions_dev [i]}, len: {len(predictions_dev [i].split())}\")\n",
        "    print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5XkHeUDrZe5"
      },
      "source": [
        "## Computar o Bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wkz2J31xcMe"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import math\n",
        "\n",
        "\n",
        "def _get_ngrams(segment, max_order):\n",
        "    \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n",
        "\n",
        "    Args:\n",
        "      segment: text segment from which n-grams will be extracted.\n",
        "      max_order: maximum length in tokens of the n-grams returned by this\n",
        "          methods.\n",
        "\n",
        "    Returns:\n",
        "      The Counter containing all n-grams upto max_order in segment\n",
        "      with a count of how many times each n-gram occurred.\n",
        "    \"\"\"\n",
        "    ngram_counts = collections.Counter()\n",
        "    for order in range(1, max_order + 1):\n",
        "        for i in range(0, len(segment) - order + 1):\n",
        "            ngram = tuple(segment[i:i + order])\n",
        "            ngram_counts[ngram] += 1\n",
        "    return ngram_counts\n",
        "\n",
        "\n",
        "def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
        "                 smooth=False):\n",
        "    \"\"\"Computes BLEU score of translated segments against one or more references.\n",
        "\n",
        "    Args:\n",
        "      reference_corpus: list of lists of references for each translation. Each\n",
        "          reference should be tokenized into a list of tokens.\n",
        "      translation_corpus: list of translations to score. Each translation\n",
        "          should be tokenized into a list of tokens.\n",
        "      max_order: Maximum n-gram order to use when computing BLEU score.\n",
        "      smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
        "\n",
        "    Returns:\n",
        "      3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
        "      precisions and brevity penalty.\n",
        "    \"\"\"\n",
        "    matches_by_order = [0] * max_order\n",
        "    possible_matches_by_order = [0] * max_order\n",
        "    reference_length = 0\n",
        "    translation_length = 0\n",
        "    for (references, translation) in zip(reference_corpus,\n",
        "                                         translation_corpus):\n",
        "        reference_length += min(len(r) for r in references)\n",
        "        translation_length += len(translation)\n",
        "\n",
        "        merged_ref_ngram_counts = collections.Counter()\n",
        "        for reference in references:\n",
        "            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
        "        translation_ngram_counts = _get_ngrams(translation, max_order)\n",
        "        overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
        "        for ngram in overlap:\n",
        "            matches_by_order[len(ngram) - 1] += overlap[ngram]\n",
        "        for order in range(1, max_order + 1):\n",
        "            possible_matches = len(translation) - order + 1\n",
        "            if possible_matches > 0:\n",
        "                possible_matches_by_order[order - 1] += possible_matches\n",
        "\n",
        "    precisions = [0] * max_order\n",
        "    for i in range(0, max_order):\n",
        "        if smooth:\n",
        "            precisions[i] = ((matches_by_order[i] + 1.) /\n",
        "                             (possible_matches_by_order[i] + 1.))\n",
        "        else:\n",
        "            if possible_matches_by_order[i] > 0:\n",
        "                precisions[i] = (float(matches_by_order[i]) /\n",
        "                                 possible_matches_by_order[i])\n",
        "            else:\n",
        "                precisions[i] = 0.0\n",
        "\n",
        "    if min(precisions) > 0:\n",
        "        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
        "        geo_mean = math.exp(p_log_sum)\n",
        "    else:\n",
        "        geo_mean = 0\n",
        "\n",
        "    ratio = float(translation_length) / reference_length\n",
        "\n",
        "    if ratio > 1.0:\n",
        "        bp = 1.\n",
        "    else:\n",
        "        bp = math.exp(1 - 1. / ratio)\n",
        "\n",
        "    bleu = geo_mean * bp\n",
        "\n",
        "    return bleu, precisions, bp, ratio, translation_length, reference_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WK0QxeF0OYZ"
      },
      "outputs": [],
      "source": [
        "def remove_padding(tokens_list):\n",
        "    \"\"\"\n",
        "    Remove padding tokens (extra spaces) from a list of tokenized sequences.\n",
        "    \"\"\"\n",
        "    return [seq.strip() for seq in tokens_list]\n",
        "\n",
        "\n",
        "def calculate_bleu_with_padding_removal(predictions, targets, max_order=4):\n",
        "    \"\"\"\n",
        "    Calculate the BLEU score for sequences with padding tokens removed.\n",
        "    \"\"\"\n",
        "    # Remove padding tokens\n",
        "    cleaned_predictions = remove_padding(predictions)\n",
        "    cleaned_targets = remove_padding(targets)\n",
        "\n",
        "    # Tokenize the cleaned sequences\n",
        "    tokenized_predictions = [prediction.split() for prediction in cleaned_predictions]\n",
        "    tokenized_targets = [target.split() for target in cleaned_targets]\n",
        "\n",
        "    # Convert each target into a list containing a single list (as expected by the compute_bleu function).\n",
        "    reference_corpus = [[target] for target in tokenized_targets]\n",
        "\n",
        "    # Compute the BLEU score\n",
        "    bleu_score, precisions, bp, ratio, translation_length, reference_length = compute_bleu(reference_corpus, tokenized_predictions, max_order=max_order, smooth=True)\n",
        "\n",
        "    return bleu_score, precisions, bp, ratio, translation_length, reference_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RSmUgvKkh8p"
      },
      "outputs": [],
      "source": [
        "# Calculate BLEU for dev and test sets for BLEU-1 to BLEU-4, with padding tokens removed\n",
        "bleu_results = {}\n",
        "for dataset_name, (predictions, targets) in [('dev', (predictions_dev, targets_dev)), ('test', (predictions_test, targets_test))]:\n",
        "    bleu_results[dataset_name] = {}\n",
        "    for n in range(1, 5):  # For BLEU-1 to BLEU-4\n",
        "        bleu_score, precisions, bp, ratio, translation_length, reference_length = calculate_bleu_with_padding_removal(predictions, targets, max_order=n)\n",
        "        bleu_results[dataset_name][f'BLEU-{n}'] = bleu_score\n",
        "bleu_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_2LjO9C7hQ1"
      },
      "outputs": [],
      "source": [
        "# Registro no wandb\n",
        "bleu_4_dev = bleu_results['dev']['BLEU-4']\n",
        "bleu_4_test = bleu_results['test']['BLEU-4']\n",
        "\n",
        "wandb.log({\n",
        "    \"BLEU-4/dev\": bleu_4_dev,\n",
        "    \"BLEU-4/test\": bleu_4_test,\n",
        "})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "b48zhu0KxcMO",
        "skoAArDhxcMS",
        "Dc93Gvz0dmsD",
        "84uf4ImgxcMT"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f938122b5c6c4ad8b397ababbd0b2390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d2872e0ee904d88a8934cf0b4280d78",
              "IPY_MODEL_96f63b3371444414a3fded94f5b7bd29",
              "IPY_MODEL_8be432fe40894001a4786e04e19e3e87"
            ],
            "layout": "IPY_MODEL_4693d5d2894746e9987dce61f38759e8"
          }
        },
        "8d2872e0ee904d88a8934cf0b4280d78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5ded3649a194f6d82efcccf011f012e",
            "placeholder": "​",
            "style": "IPY_MODEL_4c5ded9a701c4672a938fc4d6055495c",
            "value": "Extraindo: 100%"
          }
        },
        "96f63b3371444414a3fded94f5b7bd29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7958d4ddb43143fb9536d1fc39cb91cb",
            "max": 2684462841,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c19672d3d5444a0905dadff39f977a4",
            "value": 2684462841
          }
        },
        "8be432fe40894001a4786e04e19e3e87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae503f0b24ff4fb5ae0cdf87d75cd8c9",
            "placeholder": "​",
            "style": "IPY_MODEL_311496315ec8492b9a15d0827753c797",
            "value": " 2.68G/2.68G [00:25&lt;00:00, 106MB/s]"
          }
        },
        "4693d5d2894746e9987dce61f38759e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5ded3649a194f6d82efcccf011f012e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c5ded9a701c4672a938fc4d6055495c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7958d4ddb43143fb9536d1fc39cb91cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c19672d3d5444a0905dadff39f977a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae503f0b24ff4fb5ae0cdf87d75cd8c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "311496315ec8492b9a15d0827753c797": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}